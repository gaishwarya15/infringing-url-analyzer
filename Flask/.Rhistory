install.packages("jsonlite")
install.packages("httr")
install.packages("curl")
install.packages("dplyr")
install.packages("ggplot2")
install.packages("parallel")
library(jsonlite)
library(httr)
library(curl)
library(ggplot2)
library(dplyr)
library(parallel)
get_ip_address <- function(url) {
tryCatch({
hostname <- parse_url(url)$hostname
if (is.null(hostname) || hostname == "") {
return(NA)  # Return NA if hostname extraction fails
}
ip_address <- curl::nslookup(hostname)
if (length(ip_address) > 0) {
return(ip_address[1])  # Return the first IP address
} else {
return(NA)  # Return NA if no IP address found
}
}, error = function(e) {
message(paste("Error resolving URL:", url, "Error:", e$message))
return(NA)  # Return NA if an error occurs
})
}
# Load the JSON data from the file
json_file <- "D:/Flask/response.json"
json_data <- fromJSON(json_file)
# Extract the 'works' list from the JSON data
works_list <- json_data$notices$works
# Initialize an empty vector to store all infringing URLs
all_infringing_urls <- c()
# Loop through each work and collect infringing URLs
for (work in works_list) {
if (!is.null(work$infringing_urls)) {
urls <- sapply(work$infringing_urls, function(x) x$url)
all_infringing_urls <- c(all_infringing_urls, urls)
}
}
# Remove duplicates if needed
all_infringing_urls <- unique(all_infringing_urls)
# Print the variable containing all infringing URLs
# print(all_infringing_urls)
# Flatten the list of URLs
flattened_urls <- unlist(all_infringing_urls, use.names = FALSE)
# Remove duplicate URLs to avoid redundant IP resolution
unique_urls <- unique(flattened_urls)
# Define batch size
batch_size <- 1000  # Adjust based on your memory capacity and processing speed
# Function to process a batch of URLs
process_batch <- function(urls) {
result <- sapply(urls, get_ip_address)
# Debug output
message("Processed batch with ", length(urls), " URLs.")
return(result)
}
# Number of cores to use for parallel processing
num_cores <- 4  # Set this to the number of cores you want to use
# Split URLs into batches
num_batches <- ceiling(length(unique_urls) / batch_size)
batches <- split(unique_urls, ceiling(seq_along(unique_urls) / batch_size))
# Set up parallel processing
cl <- makeCluster(num_cores)
clusterExport(cl, c("batches", "process_batch", "get_ip_address", "parse_url", "curl"))
install.packages("curl")
clusterEvalQ(cl, library(curl))
# Process batches in parallel
results <- parLapply(cl, batches, process_batch)
# Stop the cluster
stopCluster(cl)
# Combine results
all_urls <- unlist(unique_urls)
all_ips <- unlist(results)
# Create a data frame with the results
df <- data.frame(
URL = all_urls,
IP_Address = all_ips,
stringsAsFactors = FALSE
)
# Display the first 10 URLs and their corresponding IP addresses
print(head(df, 10))
write.csv(df, "url_data_analysis_rmarkdown_output.csv", row.names = FALSE)
print("CSV file created and saved to 'url_data_analysis_rmarkdown_output.csv'.")
# Count the frequency of each IP address
ip_counts <- df %>%
count(IP_Address, sort = TRUE)
# Plot the top 10 most frequent IP addresses
top_10_ips <- ip_counts %>%
top_n(n = 10, wt = n)
ggplot(top_10_ips, aes(x = reorder(IP_Address, n), y = n)) +
geom_bar(stat = "identity", fill = "lightslategrey") +
coord_flip() +
labs(title = "Top 10 Most Frequent IP Addresses",
x = "IP Address",
y = "Frequency") +
theme_minimal()
if (!require(urltools)) {
install.packages("urltools")
library(urltools)
}
all_infringing_urls <- unlist(all_infringing_urls)
domains <- urltools::domain(all_infringing_urls)
# Count the number of unique domains
num_unique_domains <- length(unique(domains))
# Print the number of unique domains
print(paste("Number of unique domains:",num_unique_domains))
print(length(all_infringing_urls))
install.packages("jsonlite")
install.packages("httr")
install.packages("curl")
install.packages("dplyr")
install.packages("ggplot2")
install.packages("parallel")
library(jsonlite)
library(httr)
library(curl)
library(ggplot2)
library(dplyr)
library(parallel)
get_ip_address <- function(url) {
tryCatch({
hostname <- parse_url(url)$hostname
if (is.null(hostname) || hostname == "") {
return(NA)  # Return NA if hostname extraction fails
}
ip_address <- curl::nslookup(hostname)
if (length(ip_address) > 0) {
return(ip_address[1])  # Return the first IP address
} else {
return(NA)  # Return NA if no IP address found
}
}, error = function(e) {
message(paste("Error resolving URL:", url, "Error:", e$message))
return(NA)  # Return NA if an error occurs
})
}
# Load the JSON data from the file
json_file <- "D:/Flask/response.json"
json_data <- fromJSON(json_file)
# Extract the 'works' list from the JSON data
works_list <- json_data$notices$works
install.packages("ggplot2")
# Initialize an empty vector to store all infringing URLs
all_infringing_urls <- c()
# Loop through each work and collect infringing URLs
for (work in works_list) {
if (!is.null(work$infringing_urls)) {
urls <- sapply(work$infringing_urls, function(x) x$url)
all_infringing_urls <- c(all_infringing_urls, urls)
}
}
# Remove duplicates if needed
all_infringing_urls <- unique(all_infringing_urls)
# Print the variable containing all infringing URLs
# print(all_infringing_urls)
# Flatten the list of URLs
flattened_urls <- unlist(all_infringing_urls, use.names = FALSE)
# Remove duplicate URLs to avoid redundant IP resolution
unique_urls <- unique(flattened_urls)
# Define batch size
batch_size <- 1000  # Adjust based on your memory capacity and processing speed
# Function to process a batch of URLs
process_batch <- function(urls) {
result <- sapply(urls, get_ip_address)
# Debug output
message("Processed batch with ", length(urls), " URLs.")
return(result)
}
# Number of cores to use for parallel processing
num_cores <- 4  # Set this to the number of cores you want to use
# Split URLs into batches
num_batches <- ceiling(length(unique_urls) / batch_size)
batches <- split(unique_urls, ceiling(seq_along(unique_urls) / batch_size))
# Set up parallel processing
cl <- makeCluster(num_cores)
clusterExport(cl, c("batches", "process_batch", "get_ip_address", "parse_url", "curl"))
install.packages("dplyr")
clusterEvalQ(cl, library(curl))
# Process batches in parallel
results <- parLapply(cl, batches, process_batch)
install.packages("jsonlite")
install.packages("httr")
install.packages("curl")
install.packages("dplyr")
install.packages("ggplot2")
install.packages("parallel")
library(jsonlite)
library(httr)
library(curl)
library(ggplot2)
library(dplyr)
library(parallel)
get_ip_address <- function(url) {
tryCatch({
hostname <- parse_url(url)$hostname
if (is.null(hostname) || hostname == "") {
return(NA)  # Return NA if hostname extraction fails
}
ip_address <- curl::nslookup(hostname)
if (length(ip_address) > 0) {
return(ip_address[1])  # Return the first IP address
} else {
return(NA)  # Return NA if no IP address found
}
}, error = function(e) {
message(paste("Error resolving URL:", url, "Error:", e$message))
return(NA)  # Return NA if an error occurs
})
}
# Load the JSON data from the file
json_file <- "D:/Flask/response.json"
json_data <- fromJSON(json_file)
# Extract the 'works' list from the JSON data
works_list <- json_data$notices$works
# Initialize an empty vector to store all infringing URLs
all_infringing_urls <- c()
# Loop through each work and collect infringing URLs
for (work in works_list) {
if (!is.null(work$infringing_urls)) {
urls <- sapply(work$infringing_urls, function(x) x$url)
all_infringing_urls <- c(all_infringing_urls, urls)
}
}
# Remove duplicates if needed
all_infringing_urls <- unique(all_infringing_urls)
# Print the variable containing all infringing URLs
# print(all_infringing_urls)
# Flatten the list of URLs
flattened_urls <- unlist(all_infringing_urls, use.names = FALSE)
# Remove duplicate URLs to avoid redundant IP resolution
unique_urls <- unique(flattened_urls)
# Define batch size
batch_size <- 1000  # Adjust based on your memory capacity and processing speed
# Function to process a batch of URLs
process_batch <- function(urls) {
result <- sapply(urls, get_ip_address)
# Debug output
message("Processed batch with ", length(urls), " URLs.")
return(result)
}
# Number of cores to use for parallel processing
num_cores <- 4  # Set this to the number of cores you want to use
# Split URLs into batches
num_batches <- ceiling(length(unique_urls) / batch_size)
batches <- split(unique_urls, ceiling(seq_along(unique_urls) / batch_size))
# Set up parallel processing
cl <- makeCluster(num_cores)
clusterExport(cl, c("batches", "process_batch", "get_ip_address", "parse_url", "curl"))
clusterEvalQ(cl, library(curl))
# Process batches in parallel
results <- parLapply(cl, batches, process_batch)
install.packages("curl")
# Stop the cluster
stopCluster(cl)
# Combine results
all_urls <- unlist(unique_urls)
all_ips <- unlist(results)
# Create a data frame with the results
df <- data.frame(
URL = all_urls,
IP_Address = all_ips,
stringsAsFactors = FALSE
)
# Display the first 10 URLs and their corresponding IP addresses
print(head(df, 10))
write.csv(df, "rmarkdown_output.csv", row.names = FALSE)
print("CSV file created and saved to 'rmarkdown_output.csv'.")
# Count the frequency of each IP address
ip_counts <- df %>%
count(IP_Address, sort = TRUE)
# Top 10 most frequent IP addresses
top_10_ips <- ip_counts %>%
top_n(n = 10, wt = n)
ggplot(top_10_ips, aes(x = reorder(IP_Address, n), y = n)) +
geom_bar(stat = "identity", fill = "lightslategrey") +
coord_flip() +
labs(title = "Top 10 Most Frequent IP Addresses",
x = "IP Address",
y = "Frequency") +
theme_minimal()
if (!require(urltools)) {
install.packages("urltools")
library(urltools)
}
all_infringing_urls <- unlist(all_infringing_urls)
domains <- urltools::domain(all_infringing_urls)
# Count the number of unique domains
num_unique_domains <- length(unique(domains))
# Print the number of unique domains
print(paste("Number of unique domains:",num_unique_domains))
print(length(all_infringing_urls))
install.packages("jsonlite")
install.packages("httr")
install.packages("curl")
install.packages("dplyr")
install.packages("ggplot2")
install.packages("parallel")
install.packages("dplyr")
install.packages("ggplot2")
install.packages("curl")
install.packages("jsonlite")
install.packages("httr")
